{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7lVT9xxpYBx",
        "outputId": "b7a04ba3-37f8-45c0-fb29-25eaa1eead73"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/mwbaj/MachineLearning/ZPS_2023_winter/WAWTPC/io_functions.py\n",
        "!pip install uproot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4H-N0q-KkFD",
        "outputId": "cdf2384e-a4a7-4274-81e9-5d42c6fc4f15"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "import tensorflow_datasets as tfds\n",
        "import io_functions as io\n",
        "from tensorflow.data import Dataset, TFRecordDataset\n",
        "from tensorflow.io import TFRecordWriter, TFRecordOptions\n",
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Example, Features, Feature\n",
        "from multiprocessing import Process, Queue\n",
        "from os.path import isfile\n",
        "\n",
        "\n",
        "np_config.enable_numpy_behavior()\n",
        "drive.mount('/content/drive')\n",
        "dataPath = 'drive/MyDrive/ZPS/simulated_data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxAMggC1nHhJ"
      },
      "outputs": [],
      "source": [
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def serialize(charge_array, target):\n",
        "  feature = {'myChargeArray' : _bytes_feature(tf.io.serialize_tensor(charge_array)),\n",
        "             'target' : _bytes_feature(tf.io.serialize_tensor(target))}\n",
        "  example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "  return example.SerializeToString()\n",
        "\n",
        "def XYZtoUVWT(data):\n",
        "    referencePoint = tf.constant([-138.9971, 98.25])\n",
        "    phi = np.pi/6.0\n",
        "    stripPitch = 1.5\n",
        "    f = 1.0/25*6.46\n",
        "    u = -(data[:, 1]-99.75)\n",
        "    v = (data[:, 0]-referencePoint[0]) * np.cos(phi) - (data[:, 1]-referencePoint[1]) * np.sin(phi)\n",
        "    w = (data[:, 0]-referencePoint[0]) * np.cos(-phi) - (data[:, 1]-referencePoint[1]) * np.sin(-phi) + 98.75\n",
        "    t = data[:, 2]/f + 256\n",
        "    u/=stripPitch\n",
        "    v/=stripPitch\n",
        "    w/=stripPitch\n",
        "    return tf.stack([u,v,w,t], axis=0).T\n",
        "\n",
        "def conversion(filename, queue):\n",
        "    options = TFRecordOptions(compression_type='GZIP')\n",
        "    writer = TFRecordWriter(filename, options=options)\n",
        "    scale = 100\n",
        "    n_projections = 3\n",
        "    while True:\n",
        "        item = queue.get()\n",
        "        if item == None:\n",
        "            break\n",
        "        myChargeArray, target = item\n",
        "        charge_array= io.proc_features(myChargeArray)\n",
        "        charge_array = tf.transpose(charge_array, perm = [0, 3, 1, 2])\n",
        "        uvwt_1 = XYZtoUVWT(scale*target[:, 0:3])\n",
        "        uvwt_2 = XYZtoUVWT(scale*target[:, 3:6])\n",
        "        uvwt_3 = XYZtoUVWT(scale*target[:, 6:9])\n",
        "\n",
        "        points = []\n",
        "        for i in range(n_projections):\n",
        "          points.append([\n",
        "              uvwt_1[:, 3], uvwt_1[:, i],\n",
        "              uvwt_2[:, 3], uvwt_2[:, i],\n",
        "              uvwt_3[:, 3], uvwt_3[:, i]\n",
        "          ])\n",
        "        points = np.stack(points, axis = 1).T\n",
        "        for index in range(points.shape[0]):\n",
        "          example = serialize(charge_array[index], points[index])\n",
        "          writer.write(example)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLr14QCjEeU1"
      },
      "outputs": [],
      "source": [
        "def process_file(output_files, datasetGenerator):\n",
        "  nFiles = len(output_files)\n",
        "  for file in output_files:\n",
        "      if isfile(file):\n",
        "          raise Exception('output file already exists')\n",
        "  if __name__ == '__main__':\n",
        "      processes = []\n",
        "      q = Queue(2*nFiles)\n",
        "      for name in output_files:\n",
        "          p = Process(target=conversion, args=(name, q))\n",
        "          processes.append(p)\n",
        "          p.start()\n",
        "          print(p.name + ' started')\n",
        "\n",
        "      counter = 0\n",
        "      for item in datasetGenerator:\n",
        "          q.put(item)\n",
        "          counter+=1\n",
        "          if counter%100 == 0:\n",
        "              print(f'read {counter} batches')\n",
        "\n",
        "      for _ in range(nFiles):\n",
        "          q.put(None)\n",
        "\n",
        "      for p in processes:\n",
        "          p.join()\n",
        "          print(p.name + ' done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJHzEJGMnaSj",
        "outputId": "65861bda-fc18-4d76-a31e-e2c66ba2683a"
      },
      "outputs": [],
      "source": [
        "input_files = [dataPath+'out_random_sigma-001.root:TPCData']\n",
        "batchSize = 5\n",
        "nFiles = 5\n",
        "output_files = [dataPath + 'test/' + f\"out_random_sigma-001-part-{i}.tfrecord\" for i in range(nFiles)]\n",
        "datasetGenerator = io.minimal_generator(files=input_files, batchSize=batchSize)\n",
        "process_file(output_files, datasetGenerator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuqAi5d0Etsx"
      },
      "source": [
        "# Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj83JHNnqMFs"
      },
      "outputs": [],
      "source": [
        "filenames = [dataPath + 'test/' + f\"out_random_sigma2k2mm-part-{i}.tfrecord\" for i in range(nFiles)]\n",
        "train_dataset = tf.data.TFRecordDataset(filenames, compression_type='GZIP', num_parallel_reads=5)\n",
        "# Create a description of the features.\n",
        "feature_description = {\n",
        "    'myChargeArray': tf.io.FixedLenFeature([], tf.string),\n",
        "    'target': tf.io.FixedLenFeature([], tf.string),\n",
        "\n",
        "}\n",
        "\n",
        "def _parse_function(example_proto):\n",
        "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
        "    parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
        "    charge, target = parsed_features['myChargeArray'], parsed_features['target']\n",
        "    # decode from bytes\n",
        "    charge = tf.io.parse_tensor(charge, tf.float64)\n",
        "    target = tf.io.parse_tensor(target, tf.float64)\n",
        "\n",
        "    return charge, target\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "XQ-_7KCCzvUF",
        "outputId": "b2df509c-f155-4e55-bf59-464c45469507"
      },
      "outputs": [],
      "source": [
        "for image, target in train_dataset.take(1):\n",
        "  print(image.shape)\n",
        "  points = target.reshape(3, 3, 2)\n",
        "  plt.imshow(image[2, :, :])\n",
        "  plt.scatter(points[2, :, 0], points[2, :, 1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
